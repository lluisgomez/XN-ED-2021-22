{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JGr-h1xvTOZ"
   },
   "source": [
    "# MNIST Simple Pointer Network\n",
    "\n",
    "In this notebook we will learn how to create a simple [Pointer Network](https://arxiv.org/abs/1506.03134) (Vinyals et al. 2015) for solving a dummy task on the MNIST dataset.\n",
    "\n",
    "A Pointer Network uses the **attention mechanism’s** output to model the conditional probability of each element on its input. This can be extremely useful in tasks that require selecting one (or more) elements of the input sequence/set to be solved.\n",
    "\n",
    "In this notebook we will play with a simple Pointer net to solve the following task: Given an MNIST image with a (query) digit we want our model to find the image that contains the consecutive digit to the query image among a set of input images. For example, imagine we use an image with the digit \"5\" as query and let the input of the model be a set of 10 images with different digits, the output of our model must be a probability distribution over those 10 images indicating the presence or not of the digit \"6\" in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZuI62-_ABu-",
    "outputId": "fe0c7e11-612f-44f0-fd83-620ef7f57fb2"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyM0DXter9Mp"
   },
   "source": [
    "### Create the dataset\n",
    "\n",
    "Let's start creating a Dataset class to understand better the task we want to solve. Each sample in out dataset will be formed by:\n",
    "\n",
    "* a query image\n",
    "* a set of 10 images, 9 of them selected randomly (distractors) and one selected as the consecutive digit of the digit in the query image.\n",
    "* the ground truth: a 10-D one hot vector indicating the position of the image we want our model to select (\"point to\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XksdVS7GHWw0"
   },
   "outputs": [],
   "source": [
    "class MNISTAttentionDataset(Dataset):\n",
    "    \"\"\"MNIST attention toy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_inputs (int) - The number of images in the input of our model.\n",
    "            train (bool, optional) – If True, creates dataset from MNIST training\n",
    "                samples, otherwise from test\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.mnist = datasets.MNIST('../data', train=train, download=True)\n",
    "        \n",
    "        # dict with samples for each class label\n",
    "        self.data = {}\n",
    "        for label in range(10):\n",
    "            self.data[label] = self.mnist.data[self.mnist.targets == label] / 255.\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mnist.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        c = np.random.randint(0,10)\n",
    "        idx = np.random.randint(0, self.data[c].shape[0])\n",
    "        query = self.data[c][idx,:].flatten()\n",
    "\n",
    "        correct_pos = np.random.randint(0,self.num_inputs)\n",
    "        \n",
    "        inputs = np.zeros((self.num_inputs, 784))\n",
    "        for j in range(self.num_inputs):\n",
    "          if j == correct_pos:\n",
    "            idx = np.random.randint(0,self.data[(c+1)%10].shape[0])\n",
    "            inputs[j,:] = self.data[(c+1)%10][idx,:].flatten()\n",
    "          else:\n",
    "            c_distractor = np.random.choice([n for n in [0,1,2,3,4,5,6,7,8,9] if n!=(c+1)%10])\n",
    "            idx = np.random.randint(0,self.data[c_distractor].shape[0])\n",
    "            inputs[j,:] = self.data[c_distractor][idx,:].flatten()\n",
    "            \n",
    "        sample = {'x': inputs, 'query': query, 'y': correct_pos}\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL2kwN-kpjUc"
   },
   "outputs": [],
   "source": [
    "num_inputs = 10\n",
    "\n",
    "train_dataset = MNISTAttentionDataset(num_inputs, train=True)\n",
    "test_dataset = MNISTAttentionDataset(num_inputs, train=False)\n",
    "\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs  = {'batch_size': 1000}\n",
    "\n",
    "cuda_kwargs = {'num_workers': 0,\n",
    "               'pin_memory': True,\n",
    "               'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
    "test_loader  = DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RES5R0CMr3bE"
   },
   "source": [
    "### Visualize one training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "fJY6zR_lqR41",
    "outputId": "789330a8-92bb-4445-dc72-9c4635342ecc"
   },
   "outputs": [],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "print(sample['x'].shape, sample['query'].shape)\n",
    "\n",
    "fig = plt.figure(figsize=(33, 3))\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = plt.subplot(1, num_inputs + 1, 1)\n",
    "image = sample['query'].reshape(28,28)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.plot([0,27,27,0,0], [0,0,27,27,0], c='b')\n",
    "ax.set_title('Query')\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(num_inputs):\n",
    "    ax = plt.subplot(1, num_inputs + 1, i+2)\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    image = sample['x'][i].reshape(28,28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    if i == sample['y']:\n",
    "      plt.plot([0,27,27,0,0], [0,0,27,27,0], c='g')\n",
    "      ax.set_title('Target')\n",
    "    else:\n",
    "      plt.plot([0,27,27,0,0], [0,0,27,27,0], c='r')\n",
    "      ax.set_title('Distractor #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "  \n",
    "\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVaMM-n2sFF6"
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BesD8B1BAW9u"
   },
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_inputs, hidden_dim):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_inputs = num_inputs\n",
    "        self.input_dim  = input_dim\n",
    "\n",
    "        # Weights (Fully Connected layers) \n",
    "        self.fc_q = nn.Linear(input_dim, hidden_dim) # Query FC\n",
    "        self.fc_k = nn.Linear(input_dim, hidden_dim) # Keys FC. Here the input acts as the keys, and there is no value\n",
    "        self.fc_v = nn.Linear(hidden_dim, 1) # Values FC\n",
    "\n",
    "    def scoringAdditive(self, query, keys):\n",
    "        #Query is                                               (B X 1 X input_dim)\n",
    "        # Repeat (tile) the query so that it has the same size as the keys (input)\n",
    "        query = query.repeat(1, self.num_inputs, 1)           # (B X num_inputs X input_dim)\n",
    "        query = torch.tanh(self.fc_q(query))                  # (B X num_inputs X hidden_dim)\n",
    "\n",
    "        #Keys is                                                (B X num_inputs X input_dim)\n",
    "        keys = torch.tanh(self.fc_k(keys))                    # (B X num_inputs X hidden_dim)\n",
    "        score = torch.tanh(query + keys )                     # (B X num_inputs X hidden_dim)\n",
    "        score = self.fc_v(score)                              # (B X num_inputs X 1)        \n",
    "        return score\n",
    "\n",
    "    def forward(self, x, query):\n",
    "        query = query.unsqueeze(1) # (B x input_dim) -> (B x 1 x input_dim)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        output = self.scoringAdditive(query, x)\n",
    "        output = output.squeeze()       # (B x 1 x num_inputs) -> (B x num_inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_yCyvgg9Pbj"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "**QUESTION**: The Additive score gives logits that we should pass through a softmax to get the final attention weights. Why is there no softmax in the model above?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "    \n",
    "**ANSWER**: Your Answer Here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsyUZE0lHJ5-",
    "outputId": "218369fc-efcb-4a62-9e29-5af570371c53"
   },
   "outputs": [],
   "source": [
    "input_dim, num_inputs, hidden_dim = (784, 10, 256)\n",
    "\n",
    "model = AttentionModel(input_dim, num_inputs, hidden_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmp0kuk2z3Rx"
   },
   "source": [
    "### Define the training method and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZvGWqwDrc-V"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    # Method to perform one epoch of training\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    for batch_idx, sample_batched in enumerate(train_loader):\n",
    "        data = sample_batched['x'].float()\n",
    "        query = sample_batched['query'].float()\n",
    "        target = sample_batched['y']\n",
    "        #print(data.shape, query.shape)\n",
    "        data, query, target = data.to(device), query.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, query)\n",
    "        loss = criterion(output, target)\n",
    "        loss_values.append(loss.detach().cpu().numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_ZTvDKRz9yx",
    "outputId": "a9aa7589-4464-4328-8ccf-c3deb862feaf"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "log_interval = 100 # how many batches to wait before logging training status\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_values = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    loss_history += loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "V1VFBe_Y0Cxx",
    "outputId": "e5fbd475-41a9-4e8a-b361-c3da0e2c0802"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jONlgxyaEwC6"
   },
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izgtCdk5DV01"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total = 0.\n",
    "    ok = 0.\n",
    "    for batch_idx, sample_batched in enumerate(test_loader):\n",
    "        data = sample_batched['x'].float()\n",
    "        query = sample_batched['query'].float()\n",
    "        target = sample_batched['y'].numpy()\n",
    "        data, query = data.to(device), query.to(device)\n",
    "        \n",
    "        output = model(data, query)\n",
    "        pred = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "       \n",
    "        ok += np.sum(pred == target)\n",
    "        total += len(test_loader)\n",
    "                \n",
    "    print('Accuracy = ', ok/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYQGuAXpGNU7",
    "outputId": "e59b247d-bbff-4c84-d482-c8d48c8aea10"
   },
   "outputs": [],
   "source": [
    "evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaOoDxjhhjxz"
   },
   "source": [
    "<font color=\"blue\">\n",
    "\n",
    "**EXERCISE 1**: Starting from the previous model as a template, implement the Dot-product Attention scoring function and substitute the additive one we were using before. Retrain and evalute the model.</font>\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "> Hint: To transpose a tensor along specific dimensions you can use `torch.permute()`\n",
    "</font>\n",
    "\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "> Hint: To perform a batch matrix-matrix product, use the function `torch.bmm()` https://pytorch.org/docs/stable/generated/torch.bmm.html \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MNIST_PointerNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
